{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "Hyperparameter+optimization+on+sklearn+with+SMAC+and+GP.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/polivio/ML/blob/master/Hyperparameter%2Boptimization%2Bon%2Bsklearn%2Bwith%2BSMAC%2Band%2BGP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpJ1UGPfB_wG",
        "colab_type": "text"
      },
      "source": [
        "Use of hyperparameter optimization for choosing the best algorithm and hyperparameters for the KDD BR datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMq99LrHIl39",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCwdg3NKB_wH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://www.kaggle.com/c/kddbr-2019/data (KDD BR Author: Polivio)\n",
        "\n",
        "# Hyperparameter optimization (reuse of Wang Chi) \n",
        "\n",
        "import numpy as np                    # for numerical computation\n",
        "import pandas as pd                   # for data wrangling\n",
        "import matplotlib.pyplot as plt       # for plotting\n",
        "%matplotlib inline \n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpyBmfS8B_wL",
        "colab_type": "code",
        "outputId": "ebf562a3-cc5e-4ab4-e8e1-ab1c6452d274",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "# preprocessing # blob # raw\n",
        "train = pd.read_csv('https://github.com/polivio/PEE/raw/master/Training_grouped.csv')\n",
        "test = pd.read_csv('https://github.com/polivio/PEE/raw/master/Test_grouped.csv')\n",
        "\n",
        "\n",
        "train.info()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 80000 entries, 0 to 79999\n",
            "Data columns (total 21 columns):\n",
            "Unnamed: 0                          80000 non-null int64\n",
            "traininglabelorder.scatterplotID    80000 non-null int64\n",
            "qtde.ponto                          80000 non-null int64\n",
            "Negative                            80000 non-null int64\n",
            "Sample                              80000 non-null int64\n",
            "mean.signalX                        80000 non-null float64\n",
            "sd.signalX                          80000 non-null float64\n",
            "var.signalX                         80000 non-null float64\n",
            "kurtosis.signalX                    80000 non-null float64\n",
            "skew.signalX                        80000 non-null float64\n",
            "mean.signalY                        80000 non-null float64\n",
            "sd.signalY                          80000 non-null float64\n",
            "var.signalY                         80000 non-null float64\n",
            "kurtosis.signalY                    80000 non-null float64\n",
            "skew.signalY                        80000 non-null float64\n",
            "L                                   80000 non-null int64\n",
            "X                                   75408 non-null float64\n",
            "XY                                  60537 non-null float64\n",
            "Y                                   76289 non-null float64\n",
            "mean.silhouette                     80000 non-null float64\n",
            "traininglabelorder.score            80000 non-null float64\n",
            "dtypes: float64(15), int64(6)\n",
            "memory usage: 12.8 MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEqtbSM9JsT6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "835452ab-36f2-482e-fce4-64ad7225c24f"
      },
      "source": [
        "#train.isna().sum()\n",
        "##X                            4592\n",
        "##XY                          19463\n",
        "##Y                            3711\n",
        "\n",
        "# Dica Mateus substituição pela média\n",
        "train['X'].fillna(train['X'].mean(),inplace=True)\n",
        "train['XY'].fillna(train['XY'].mean(),inplace=True)\n",
        "train['Y'].fillna(train['XY'].mean(),inplace=True)\n",
        "\n",
        "train.info()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 80000 entries, 0 to 79999\n",
            "Data columns (total 21 columns):\n",
            "Unnamed: 0                          80000 non-null int64\n",
            "traininglabelorder.scatterplotID    80000 non-null int64\n",
            "qtde.ponto                          80000 non-null int64\n",
            "Negative                            80000 non-null int64\n",
            "Sample                              80000 non-null int64\n",
            "mean.signalX                        80000 non-null float64\n",
            "sd.signalX                          80000 non-null float64\n",
            "var.signalX                         80000 non-null float64\n",
            "kurtosis.signalX                    80000 non-null float64\n",
            "skew.signalX                        80000 non-null float64\n",
            "mean.signalY                        80000 non-null float64\n",
            "sd.signalY                          80000 non-null float64\n",
            "var.signalY                         80000 non-null float64\n",
            "kurtosis.signalY                    80000 non-null float64\n",
            "skew.signalY                        80000 non-null float64\n",
            "L                                   80000 non-null int64\n",
            "X                                   80000 non-null float64\n",
            "XY                                  80000 non-null float64\n",
            "Y                                   80000 non-null float64\n",
            "mean.silhouette                     80000 non-null float64\n",
            "traininglabelorder.score            80000 non-null float64\n",
            "dtypes: float64(15), int64(6)\n",
            "memory usage: 12.8 MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAs4xH3IK1r4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 689
        },
        "outputId": "f05a5fff-1bf0-4b0b-fdc1-92db608b1b18"
      },
      "source": [
        "train = train.loc[:,[#'qtde.ponto', \n",
        "       'mean.signalX', 'sd.signalX', 'kurtosis.signalX',\n",
        "      'mean.signalY', 'sd.signalY', \n",
        "       'kurtosis.signalY', 'L', 'X', 'XY', 'Y',\n",
        "       'mean.silhouette', 'traininglabelorder.score']]\n",
        "\n",
        "## Quando iremos usar os dados de teste?\n",
        "test = test.loc[:,[# 'qtde.ponto', \n",
        "       'mean.signalX', 'sd.signalX', 'kurtosis.signalX',\n",
        "      'mean.signalY', 'sd.signalY', \n",
        "       'kurtosis.signalY', 'L', 'X', 'XY', 'Y',\n",
        "       'mean.silhouette']]\n",
        "\n",
        "train.head(20)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mean.signalX</th>\n",
              "      <th>sd.signalX</th>\n",
              "      <th>kurtosis.signalX</th>\n",
              "      <th>mean.signalY</th>\n",
              "      <th>sd.signalY</th>\n",
              "      <th>kurtosis.signalY</th>\n",
              "      <th>L</th>\n",
              "      <th>X</th>\n",
              "      <th>XY</th>\n",
              "      <th>Y</th>\n",
              "      <th>mean.silhouette</th>\n",
              "      <th>traininglabelorder.score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3164.835616</td>\n",
              "      <td>804.757625</td>\n",
              "      <td>3.516731</td>\n",
              "      <td>1586.430137</td>\n",
              "      <td>568.651580</td>\n",
              "      <td>3.170632</td>\n",
              "      <td>4</td>\n",
              "      <td>92.0</td>\n",
              "      <td>269.000000</td>\n",
              "      <td>80.421065</td>\n",
              "      <td>0.840146</td>\n",
              "      <td>0.009647</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1737.776042</td>\n",
              "      <td>944.798845</td>\n",
              "      <td>2.475172</td>\n",
              "      <td>2999.940104</td>\n",
              "      <td>1716.790542</td>\n",
              "      <td>2.195067</td>\n",
              "      <td>4</td>\n",
              "      <td>82.0</td>\n",
              "      <td>49.000000</td>\n",
              "      <td>249.000000</td>\n",
              "      <td>0.964427</td>\n",
              "      <td>0.842084</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1139.621984</td>\n",
              "      <td>866.460225</td>\n",
              "      <td>1.922726</td>\n",
              "      <td>1836.640751</td>\n",
              "      <td>950.478945</td>\n",
              "      <td>1.436889</td>\n",
              "      <td>4</td>\n",
              "      <td>156.0</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>206.000000</td>\n",
              "      <td>0.936210</td>\n",
              "      <td>0.744601</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3609.692708</td>\n",
              "      <td>1808.877599</td>\n",
              "      <td>1.985210</td>\n",
              "      <td>2469.164062</td>\n",
              "      <td>1219.231193</td>\n",
              "      <td>1.913381</td>\n",
              "      <td>4</td>\n",
              "      <td>100.0</td>\n",
              "      <td>190.000000</td>\n",
              "      <td>90.000000</td>\n",
              "      <td>0.896070</td>\n",
              "      <td>0.842084</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1867.203655</td>\n",
              "      <td>938.232021</td>\n",
              "      <td>2.176598</td>\n",
              "      <td>2180.908616</td>\n",
              "      <td>1515.205489</td>\n",
              "      <td>1.155226</td>\n",
              "      <td>4</td>\n",
              "      <td>176.0</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>201.000000</td>\n",
              "      <td>0.952810</td>\n",
              "      <td>0.944177</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>449.072917</td>\n",
              "      <td>507.413489</td>\n",
              "      <td>29.834375</td>\n",
              "      <td>2910.351562</td>\n",
              "      <td>358.648637</td>\n",
              "      <td>7.460460</td>\n",
              "      <td>4</td>\n",
              "      <td>12.0</td>\n",
              "      <td>80.421065</td>\n",
              "      <td>368.000000</td>\n",
              "      <td>0.968487</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2076.953039</td>\n",
              "      <td>655.688216</td>\n",
              "      <td>2.774567</td>\n",
              "      <td>1170.375691</td>\n",
              "      <td>342.483082</td>\n",
              "      <td>2.222238</td>\n",
              "      <td>7</td>\n",
              "      <td>132.0</td>\n",
              "      <td>117.000000</td>\n",
              "      <td>106.000000</td>\n",
              "      <td>0.913787</td>\n",
              "      <td>0.578706</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>830.200000</td>\n",
              "      <td>648.128942</td>\n",
              "      <td>9.725987</td>\n",
              "      <td>3241.095000</td>\n",
              "      <td>693.599022</td>\n",
              "      <td>5.545654</td>\n",
              "      <td>5</td>\n",
              "      <td>3.0</td>\n",
              "      <td>22.000000</td>\n",
              "      <td>170.000000</td>\n",
              "      <td>0.916503</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>3586.317708</td>\n",
              "      <td>1862.356219</td>\n",
              "      <td>2.630635</td>\n",
              "      <td>1683.195312</td>\n",
              "      <td>1436.342429</td>\n",
              "      <td>6.643115</td>\n",
              "      <td>4</td>\n",
              "      <td>326.0</td>\n",
              "      <td>80.421065</td>\n",
              "      <td>54.000000</td>\n",
              "      <td>0.956756</td>\n",
              "      <td>0.944319</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>222.168831</td>\n",
              "      <td>270.887643</td>\n",
              "      <td>23.916200</td>\n",
              "      <td>2146.623377</td>\n",
              "      <td>434.285275</td>\n",
              "      <td>4.516220</td>\n",
              "      <td>2</td>\n",
              "      <td>6.0</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>144.000000</td>\n",
              "      <td>0.949731</td>\n",
              "      <td>0.751477</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>2238.825521</td>\n",
              "      <td>1480.077892</td>\n",
              "      <td>1.558220</td>\n",
              "      <td>1509.859375</td>\n",
              "      <td>1343.930384</td>\n",
              "      <td>2.139870</td>\n",
              "      <td>5</td>\n",
              "      <td>229.0</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>146.000000</td>\n",
              "      <td>0.955443</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>877.057292</td>\n",
              "      <td>298.678765</td>\n",
              "      <td>3.454756</td>\n",
              "      <td>496.882812</td>\n",
              "      <td>136.869587</td>\n",
              "      <td>13.971307</td>\n",
              "      <td>4</td>\n",
              "      <td>338.0</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>22.000000</td>\n",
              "      <td>0.829714</td>\n",
              "      <td>0.283536</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>901.786458</td>\n",
              "      <td>633.263041</td>\n",
              "      <td>1.963940</td>\n",
              "      <td>1232.804688</td>\n",
              "      <td>530.281231</td>\n",
              "      <td>3.126608</td>\n",
              "      <td>4</td>\n",
              "      <td>220.0</td>\n",
              "      <td>50.000000</td>\n",
              "      <td>110.000000</td>\n",
              "      <td>0.937572</td>\n",
              "      <td>0.474833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>1126.440789</td>\n",
              "      <td>697.174147</td>\n",
              "      <td>1.980671</td>\n",
              "      <td>1624.661184</td>\n",
              "      <td>637.844680</td>\n",
              "      <td>1.678215</td>\n",
              "      <td>4</td>\n",
              "      <td>140.0</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>152.000000</td>\n",
              "      <td>0.907024</td>\n",
              "      <td>0.647776</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>379.539062</td>\n",
              "      <td>320.758506</td>\n",
              "      <td>43.888449</td>\n",
              "      <td>2732.984375</td>\n",
              "      <td>379.767518</td>\n",
              "      <td>9.060104</td>\n",
              "      <td>5</td>\n",
              "      <td>4.0</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>365.000000</td>\n",
              "      <td>0.940349</td>\n",
              "      <td>0.795196</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>3469.518229</td>\n",
              "      <td>973.268089</td>\n",
              "      <td>2.523557</td>\n",
              "      <td>3425.468750</td>\n",
              "      <td>750.673319</td>\n",
              "      <td>2.913455</td>\n",
              "      <td>5</td>\n",
              "      <td>85.0</td>\n",
              "      <td>203.000000</td>\n",
              "      <td>91.000000</td>\n",
              "      <td>0.882495</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>1668.934896</td>\n",
              "      <td>797.617574</td>\n",
              "      <td>3.382641</td>\n",
              "      <td>3097.526042</td>\n",
              "      <td>817.027791</td>\n",
              "      <td>3.431970</td>\n",
              "      <td>14</td>\n",
              "      <td>18.0</td>\n",
              "      <td>272.000000</td>\n",
              "      <td>80.000000</td>\n",
              "      <td>0.485581</td>\n",
              "      <td>0.006463</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>1479.209581</td>\n",
              "      <td>969.112082</td>\n",
              "      <td>2.302135</td>\n",
              "      <td>1405.598802</td>\n",
              "      <td>907.868104</td>\n",
              "      <td>1.649250</td>\n",
              "      <td>2</td>\n",
              "      <td>76.0</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>85.000000</td>\n",
              "      <td>0.954536</td>\n",
              "      <td>0.768378</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>1505.221932</td>\n",
              "      <td>460.339956</td>\n",
              "      <td>3.421156</td>\n",
              "      <td>1128.402089</td>\n",
              "      <td>590.302079</td>\n",
              "      <td>6.052414</td>\n",
              "      <td>5</td>\n",
              "      <td>321.0</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>48.000000</td>\n",
              "      <td>0.957652</td>\n",
              "      <td>0.944177</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>1786.101604</td>\n",
              "      <td>1507.147662</td>\n",
              "      <td>1.277980</td>\n",
              "      <td>2363.534759</td>\n",
              "      <td>1498.744208</td>\n",
              "      <td>1.211140</td>\n",
              "      <td>4</td>\n",
              "      <td>157.0</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>204.000000</td>\n",
              "      <td>0.946961</td>\n",
              "      <td>0.745189</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    mean.signalX   sd.signalX  ...  mean.silhouette  traininglabelorder.score\n",
              "0    3164.835616   804.757625  ...         0.840146                  0.009647\n",
              "1    1737.776042   944.798845  ...         0.964427                  0.842084\n",
              "2    1139.621984   866.460225  ...         0.936210                  0.744601\n",
              "3    3609.692708  1808.877599  ...         0.896070                  0.842084\n",
              "4    1867.203655   938.232021  ...         0.952810                  0.944177\n",
              "5     449.072917   507.413489  ...         0.968487                  1.000000\n",
              "6    2076.953039   655.688216  ...         0.913787                  0.578706\n",
              "7     830.200000   648.128942  ...         0.916503                  1.000000\n",
              "8    3586.317708  1862.356219  ...         0.956756                  0.944319\n",
              "9     222.168831   270.887643  ...         0.949731                  0.751477\n",
              "10   2238.825521  1480.077892  ...         0.955443                  1.000000\n",
              "11    877.057292   298.678765  ...         0.829714                  0.283536\n",
              "12    901.786458   633.263041  ...         0.937572                  0.474833\n",
              "13   1126.440789   697.174147  ...         0.907024                  0.647776\n",
              "14    379.539062   320.758506  ...         0.940349                  0.795196\n",
              "15   3469.518229   973.268089  ...         0.882495                  1.000000\n",
              "16   1668.934896   797.617574  ...         0.485581                  0.006463\n",
              "17   1479.209581   969.112082  ...         0.954536                  0.768378\n",
              "18   1505.221932   460.339956  ...         0.957652                  0.944177\n",
              "19   1786.101604  1507.147662  ...         0.946961                  0.745189\n",
              "\n",
              "[20 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jq2vulQ1LAyT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "101a59cf-4842-40df-f50c-54dfa1af4b71"
      },
      "source": [
        "train.head(20)\n",
        "# change the dtype to 'float64' \n",
        "train = train.astype('float64')\n",
        "train.info()\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 80000 entries, 0 to 79999\n",
            "Data columns (total 12 columns):\n",
            "mean.signalX                80000 non-null float64\n",
            "sd.signalX                  80000 non-null float64\n",
            "kurtosis.signalX            80000 non-null float64\n",
            "mean.signalY                80000 non-null float64\n",
            "sd.signalY                  80000 non-null float64\n",
            "kurtosis.signalY            80000 non-null float64\n",
            "L                           80000 non-null float64\n",
            "X                           80000 non-null float64\n",
            "XY                          80000 non-null float64\n",
            "Y                           80000 non-null float64\n",
            "mean.silhouette             80000 non-null float64\n",
            "traininglabelorder.score    80000 non-null float64\n",
            "dtypes: float64(12)\n",
            "memory usage: 7.3 MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWzzGLmVM56G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9d3b609a-c3a9-4034-da65-2982f8d95afe"
      },
      "source": [
        "dados_nulos = train.isnull()\n",
        "print(dados_nulos)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "       mean.signalX  sd.signalX  ...  mean.silhouette  traininglabelorder.score\n",
            "0             False       False  ...            False                     False\n",
            "1             False       False  ...            False                     False\n",
            "2             False       False  ...            False                     False\n",
            "3             False       False  ...            False                     False\n",
            "4             False       False  ...            False                     False\n",
            "5             False       False  ...            False                     False\n",
            "6             False       False  ...            False                     False\n",
            "7             False       False  ...            False                     False\n",
            "8             False       False  ...            False                     False\n",
            "9             False       False  ...            False                     False\n",
            "10            False       False  ...            False                     False\n",
            "11            False       False  ...            False                     False\n",
            "12            False       False  ...            False                     False\n",
            "13            False       False  ...            False                     False\n",
            "14            False       False  ...            False                     False\n",
            "15            False       False  ...            False                     False\n",
            "16            False       False  ...            False                     False\n",
            "17            False       False  ...            False                     False\n",
            "18            False       False  ...            False                     False\n",
            "19            False       False  ...            False                     False\n",
            "20            False       False  ...            False                     False\n",
            "21            False       False  ...            False                     False\n",
            "22            False       False  ...            False                     False\n",
            "23            False       False  ...            False                     False\n",
            "24            False       False  ...            False                     False\n",
            "25            False       False  ...            False                     False\n",
            "26            False       False  ...            False                     False\n",
            "27            False       False  ...            False                     False\n",
            "28            False       False  ...            False                     False\n",
            "29            False       False  ...            False                     False\n",
            "...             ...         ...  ...              ...                       ...\n",
            "79970         False       False  ...            False                     False\n",
            "79971         False       False  ...            False                     False\n",
            "79972         False       False  ...            False                     False\n",
            "79973         False       False  ...            False                     False\n",
            "79974         False       False  ...            False                     False\n",
            "79975         False       False  ...            False                     False\n",
            "79976         False       False  ...            False                     False\n",
            "79977         False       False  ...            False                     False\n",
            "79978         False       False  ...            False                     False\n",
            "79979         False       False  ...            False                     False\n",
            "79980         False       False  ...            False                     False\n",
            "79981         False       False  ...            False                     False\n",
            "79982         False       False  ...            False                     False\n",
            "79983         False       False  ...            False                     False\n",
            "79984         False       False  ...            False                     False\n",
            "79985         False       False  ...            False                     False\n",
            "79986         False       False  ...            False                     False\n",
            "79987         False       False  ...            False                     False\n",
            "79988         False       False  ...            False                     False\n",
            "79989         False       False  ...            False                     False\n",
            "79990         False       False  ...            False                     False\n",
            "79991         False       False  ...            False                     False\n",
            "79992         False       False  ...            False                     False\n",
            "79993         False       False  ...            False                     False\n",
            "79994         False       False  ...            False                     False\n",
            "79995         False       False  ...            False                     False\n",
            "79996         False       False  ...            False                     False\n",
            "79997         False       False  ...            False                     False\n",
            "79998         False       False  ...            False                     False\n",
            "79999         False       False  ...            False                     False\n",
            "\n",
            "[80000 rows x 12 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1F40gQELYh-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "88df16fe-a4f2-4d61-a015-d3a419db0099"
      },
      "source": [
        "# feature qtde.ponto poderá ser removida??\n",
        "\n",
        "\n",
        "X = np.array(train.drop(['traininglabelorder.score'], 1))\n",
        "\n",
        "y = np.array(train['traininglabelorder.score'])\n",
        "\n",
        "\n",
        "print(y)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.00964712 0.84208443 0.74460145 ... 0.8007374  0.00594233 0.7671597 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFvIsxFlLZdi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3U0iQkZB_wO",
        "colab_type": "code",
        "outputId": "15932704-dae4-4f7b-e1ae-2a188c5611c2",
        "colab": {}
      },
      "source": [
        "                                        # raw data should be organized\n",
        "X_train, y_train, id_train, features_train = modify_data([\"time\"], \"train\")\n",
        "X_test, y_test, id_test, features_test = modify_data([\"time\"], \"test\")\n",
        "X_valid, y_valid, id_valid, features_valid = modify_data([\"time\"], \"validate\")\n",
        "X_train, y_train, id_train = remove_outliers(X_train, y_train, id_train, 1000)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The outliers are: (array([ 168,  185,  604, 1024]),)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofRhEffqB_wS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(X_train.shape)\n",
        "print(X_test.shape[0])\n",
        "print(X_valid.shape[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgmdQwPfB_wV",
        "colab_type": "text"
      },
      "source": [
        "Now, we load the necessary modules in order to build machine learning models. Most models belong to the scikit-learn package, except gradient boosted trees, where we recommend using xgboost.\n",
        "\n",
        "We consider 5 popular regression models:\n",
        "- Ridged linear regression\n",
        "- Random Forest regression\n",
        "- Gradient Boosting Tree regression\n",
        "- Support Vector Regression\n",
        "    - rbf kernel\n",
        "    - linear kernel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bii8FHnAB_wW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "# preprocessing for SVR\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.svm import LinearSVR"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgcLlo-_B_wY",
        "colab_type": "text"
      },
      "source": [
        "We fix the random seed, and assert some prelimary parameters. We will be doing a 5-fold cross validation to measure the performance of a hyperparameter configuration (we will explain later). Also, SVR needs the feature datasets to be normalized."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdilRXQiB_wZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_folds = 5\n",
        "seed = 1234\n",
        "kfold = KFold(n_splits=num_folds, random_state=seed)\n",
        "\n",
        "scoring = \"neg_mean_squared_error\"\n",
        "\n",
        "# normalize the data X_train for svr (it is a must for svr!), linear and logistic regressions.\n",
        "X_standard = StandardScaler()\n",
        "sd_X_train = X_standard.fit_transform(X_train)\n",
        "sd_X_test = X_standard.transform(X_test)\n",
        "sd_X_valid = X_standard.transform(X_valid)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXcuyB4XB_wc",
        "colab_type": "text"
      },
      "source": [
        "Ridged linear regression (denoted \"linr\"): It is the simplest model we consider. It only has one hyperparameter, alpha, which is the coefficient for the L2 penalty term in the MLE. Its hyperparameter space is the continuous interval [0.0, 100.0].\n",
        "\n",
        "For details, see http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Q5BxJZ6B_wd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "estimators = {}\n",
        "dict_spaces = {}\n",
        "\n",
        "estimators[\"linr\"] = (Ridge(solver = \"svd\"), [\"alpha\"])\n",
        "dict_spaces[\"linr\"] = {\"alpha\": [0.0, 100.0]}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCPIqURkB_wf",
        "colab_type": "text"
      },
      "source": [
        "Random Forest Regressor: This is a tree ensemble model, from which we identify the follwoing 4 hyperparameters. 4 is not a big number, and we can still explain the hyperparameters one by one:\n",
        "\n",
        "- n_estimators is the number of trees in the ensemble, \n",
        "- max_features is the number of features considered when splitting a node in a tree.\n",
        "- max_depth is the maximum depth of a tree.\n",
        "- min_sample_split is the minimum number of data points a leave has to contain. \n",
        "\n",
        "The last three features constrain the expressiveness of the model, which help to avoid over-fittng. But if they are too small, then the model might not have enough capacity to regress the wafer thickness well. Usually, we hand tune these hyperparameters to adjust the model to the \"right capacity\". But we will apply hyperparameter optimization, which automatically tune the hyperparameters.\n",
        "\n",
        "More details for random forest can be found here:\n",
        "\n",
        "http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9O0WpWyB_wg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#eg Random Forest\n",
        "estimators[\"rf\"] = (RandomForestRegressor(), [\"n_estimators\", \n",
        "                                            \"max_features\",\n",
        "                                            \"max_depth\",\n",
        "                                            \"min_samples_split\"])\n",
        "dict_spaces[\"rf\"] = {\"n_estimators\": [10, 60], \n",
        "                     \"max_features\": [\"auto\", \"sqrt\", \"log2\", 0.3, 0.4, 0.5, 0.6, 0.7, 0.8],\n",
        "                     \"max_depth\" : [None, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
        "                     \"min_samples_split\" : [2, 10]}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2SQEcOOB_wj",
        "colab_type": "text"
      },
      "source": [
        "Gradient boosted trees regressor: This is kinda close relative to random forest, but it requires more effort to tune the hyperparameters. There are quite a lot of hyperparameters, as listed. The details can be found here:\n",
        "\n",
        "http://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLLsHEDXB_wj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#eg xgb.XGBRegressor, the sklearn interface for xgboost regressor\n",
        "estimators[\"xgb\"] = (XGBRegressor(), [\"learning_rate\",   \n",
        "                                      \"gamma\",\n",
        "                                      \"max_depth\",       \n",
        "                                      \"min_child_weight\",                          \n",
        "                                      \"max_delta_step\",\n",
        "                                      \"subsample\", \n",
        "                                      \"colsample_bytree\",\n",
        "                                      \"colsample_bylevel\",\n",
        "                                      \"reg_lambda\",\n",
        "                                      \"reg_alpha\", \"n_estimators\"])\n",
        "\n",
        "dict_spaces[\"xgb\"] = {\"learning_rate\": (10**-5, 10**0, \"log-uniform\"),   \n",
        "                      \"gamma\": (0., 0.3),                          # gamma = min_split_loss\n",
        "                      \"max_depth\":  (2, 8),                     \n",
        "                      \"min_child_weight\":  (0., 3.),               # min_child_weight\n",
        "                      \"max_delta_step\" :   (0., 0.),               # max_delta_step\n",
        "                      \"subsample\":  (0.6, 1.0),                    # subsample\n",
        "                      \"colsample_bytree\" :  (0.6, 1.0),            # colsample_bytree\n",
        "                      \"colsample_bylevel\" : (0.6, 1.0),            # colsample_bylevel\n",
        "                      \"reg_lambda\" :  (0., 10.0),                  # lambda\n",
        "                      \"reg_alpha\" : (0., 2.0),                     \n",
        "                      \"n_estimators\":  (50, 600)}                  # num_rounds in xgb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4r8bo0VHB_wm",
        "colab_type": "text"
      },
      "source": [
        "Finally, we come to suppor vector regression, which has few hyperparameters. But the hyperparameters \"C\" and \"gamma\" are very very important. Also, SVR requires normalized feature data.\n",
        "\n",
        "http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html\n",
        "http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVR.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M26tSwO1B_wn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#eg Support Vector Regression with rbf kernel\n",
        "estimators[\"svr_rbf\"] =(SVR(kernel = \"rbf\"), [\"C\", \"gamma\", \"epsilon\"])\n",
        "dict_spaces[\"svr_rbf\"] = {\"C\": (10**-3, 10**4, \"log-uniform\"),   \n",
        "                          \"gamma\": [10**-4, 10**-3.5, 10**-2, 10**-1.5, 10**-1, 10**-0.5, 1.0, 10**0.5, 10**1, \"auto\"], \n",
        "                          \"epsilon\": (0., 0.5)}                       \n",
        "\n",
        "# Support Vector Regression with linear kernel\n",
        "estimators[\"svr_lin\"] = (LinearSVR(), [\"C\", \"epsilon\"])\n",
        "dict_spaces[\"svr_lin\"] = {\"C\": (10**-3, 10**4, \"log-uniform\"),   \n",
        "                          \"epsilon\": (0., 0.5)}     "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RKfgpqzB_wq",
        "colab_type": "text"
      },
      "source": [
        "We code our hyperparameter tuning algorith below. It takes as an input a sklearn model and its hyperparameter space, and outputs a good set of hyperparameter configuration by training the model over n_calls carefully selected hyperparameter configs. These configs are usually selected in a sequential manner. \n",
        "\n",
        "We employ the following 3 hyperparameter optimization algorithms:\n",
        "\n",
        "- random search\n",
        "- gaussian process optimization \n",
        "- tree ensemble regression (when random forest is used, it is SMAC)\n",
        "\n",
        "For small n_calls (say 10), these 3 algorithms roughly have the same performance. But gp and ensemble algorithms surpass random search when we have a moderate size n_calls (say 50). It is hard to say if gaussian process opt or tree ensemble is better, and my verdict is that they are roughly the same, at least when n_calls < 100. A survey on hyperparameter optimization can be found here:\n",
        "\n",
        "https://www.cs.ox.ac.uk/people/nando.defreitas/publications/BayesOptLoop.pdf\n",
        "\n",
        "We didnt include two other famous hyperparameter optimization algorithms, namely TPE and grid search. TPE kinda have similar performance to gp and tree ensemble, and we think we have enough number of hyperparameter optimization already. (We are using only one anyway). Grid search is only good when you are extremely experienced with the dataset and model, so experienced that you have a very small search space. Kagglers perform grid search on moderate size search spaces in order to get a 3rd or 4th digit improvement. But we don't think it is necessary here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eW1RxsKdB_wq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from skopt import dummy_minimize, gp_minimize, gbrt_minimize, forest_minimize\n",
        "def auto_cv_score(estimator,\n",
        "                  dict_space,\n",
        "                  X, y, \n",
        "                  kfold = kfold, \n",
        "                  scoring = scoring, \n",
        "                  default = True,\n",
        "                  hpo_optimizer = \"dummy\", \n",
        "                  n_calls = 11, \n",
        "                  n_random_starts = 10,\n",
        "                  x0=None, y0=None, \n",
        "                  random_state=seed, \n",
        "                  verbose=False):\n",
        "    \"\"\"\n",
        "        auto_cv_score aims to compute the best hyperparameter configuration for an\n",
        "        sklearn classifer/regressor, by performing a cross validation on (X, y).\n",
        "        It also works with xgboost.XGBClassifer or xgboost.XGBRegressor, which are \n",
        "        the sklearn wrappers for xgboost classifier and regressor. \n",
        "        \n",
        "        TODO: sklearn wrapper for keras??        \n",
        "        \n",
        "        Input:\n",
        "        - estimator is an ordered pair.\n",
        "            -First entry is an sklearn estimator, (we assume that we can access each \n",
        "             hyperparameter through the estimator's attribute, namely \n",
        "             \"estimator.hyperparamter\")\n",
        "            -Second entry is the ordered array of the hyperparameters in the estimator.\n",
        "             It represents the desired order in the presentation of the hyperparameter \n",
        "             configuration.\n",
        "             \n",
        "        - dict_space is a dictionary.\n",
        "            -Keys = hyperparameters' names\n",
        "            -values = the respective ranges for the hyperparameters\n",
        "            \n",
        "        - X, y are the feature data and target data.\n",
        "        \n",
        "        - kfold is the k-fold cross validation set generated by \n",
        "          sklearn.model_selection.KFold:\n",
        "            kfold = KFold(n_splits=num_folds, random_state=seed)\n",
        "            \n",
        "        - scoring is the sklearn metric used for measuring the error. By default, we use\n",
        "          scoring = \"neg_mean_squared_error\". See sklearn.metric for more details.\n",
        "          \n",
        "        - default is True, if we evaluate the cv error first with the default hyperparameter\n",
        "          config recommended by sklearn. default is False otherwise. It is recommended to \n",
        "          set it to be true, unless you are very sure on what you are doing.\n",
        "          \n",
        "        - hpo_optimizer is the hyperparameter optimizer we are using:\n",
        "            - hpo_optimizer == \"dummy\" for using random search.\n",
        "            - hpo_optimizer == \"gp\" for using gaussian process optimization.\n",
        "            - hpo_optimzier == \"gbrt\" for using the gradient boosting trees variant of SMAC.\n",
        "            - hpo_optimizer == \"rf\" for using SMAC.\n",
        "            - hpo_optimzier == \"et\" for using the extra trees variaint of SMAC.\n",
        "          In fact, the performance of the last four optimizers are kinda similar. \n",
        "          TODO: make hpo_optimizer == \"gs\" (grid search), and hpo_optimizer = \"tpe\", if\n",
        "          there is a point in making these.\n",
        "        \n",
        "        - n_calls = number of hyperparameter evaluations\n",
        "            - if default == True and hpo_optimzer != dummy, then n_calls must be \\geq 11.\n",
        "            - if default == False and hpo_optimzer != dummy, then n_calls must be \\geq 10.\n",
        "            - Otherwise, hpo_optimizer == dummy, and n_calls can be any positive integer.\n",
        "            \n",
        "        - x0 is the list of hyperparameter configs to test on before running our HPO, if \n",
        "          the corresponding y0 is empty. Otherwise, (x0, y0) serves as a warm start for \n",
        "          HPO.\n",
        "          \n",
        "        - random_state is the randome seed we are using.\n",
        "              \n",
        "            \n",
        "        Output:\n",
        "        - hpo_result: The result of the hyperparameter optimization.\n",
        "        - cv_result: The mean and std of the cv score.\n",
        "    \"\"\"\n",
        "    alg = estimator[0]\n",
        "    order = estimator[1]\n",
        "    name, space = dict_space.keys(), dict_space.values()\n",
        "    if set(order)!=set(name):\n",
        "        raise ValueError(\"The hyperparameters for the estimater and the hyperspace do not match.\")\n",
        "    idx = np.empty_like(order, dtype = \"int\")\n",
        "    order = np.asarray(order)\n",
        "    name = np.asarray(name)\n",
        "    #space = np.asarray(space)\n",
        "    for i in range(len(idx)):\n",
        "        #the index INDEX in name where name[INDEX] = order[i] = i th hyperp in order.\n",
        "        idx[i] = np.where(name==order[i])[0][0]\n",
        "    ordered_space = []\n",
        "    for i in range(len(idx)):\n",
        "        ordered_space.append(space[idx[i]])\n",
        "    if default:\n",
        "        x_default = []\n",
        "        for i in range(len(order)):\n",
        "            x_default.append(getattr(alg, order[i]))\n",
        "        if x0:\n",
        "            x0 = [x_default] + x0\n",
        "        else:\n",
        "            x0 = x_default\n",
        "    def cv_skopt(hp_vec):\n",
        "        for i in range(len(order)): #key, value in hp_dict.items():\n",
        "            setattr(alg, order[i], hp_vec[i])\n",
        "        mse = cross_val_score(alg, X, y, cv = kfold, scoring = scoring)\n",
        "        return -mse.mean() \n",
        "    if hpo_optimizer == \"dummy\":\n",
        "        hpo_result = dummy_minimize(cv_skopt, ordered_space, n_calls = n_calls, \n",
        "                                x0=x0, y0=y0, random_state=random_state, verbose=verbose)\n",
        "    elif hpo_optimizer == \"gp\":\n",
        "        hpo_result = gp_minimize(cv_skopt, ordered_space,\n",
        "                                 n_calls = n_calls, n_random_starts = n_random_starts,\n",
        "                             x0=x0, y0=y0, random_state=random_state, verbose=verbose)\n",
        "    elif hpo_optimizer == \"gbrt\":\n",
        "        hpo_result = gbrt_minimize(cv_skopt, ordered_space, \n",
        "                                   n_calls = n_calls, n_random_starts = n_random_starts,\n",
        "                             x0=x0, y0=y0, random_state=random_state, verbose=verbose)\n",
        "    elif hpo_optimizer == \"rf\":\n",
        "        hpo_result = forest_minimize(cv_skopt, ordered_space, base_estimator='RF', \n",
        "                                     n_calls = n_calls, n_random_starts = n_random_starts,\n",
        "                            x0=x0, y0=y0, random_state=random_state, verbose=verbose)\n",
        "    elif hpo_optimizer == \"et\":\n",
        "        hpo_result = forest_minimize(cv_skopt, ordered_space, base_estimator='ET', \n",
        "                                     n_calls = n_calls, n_random_starts = n_random_starts,\n",
        "                            x0=x0, y0=y0, random_state=random_state, verbose=verbose)\n",
        "    else:\n",
        "        raise ValueError(\"The hpo_optimizer must be one of the followings: dummy, gp, gbrt, rf, et.\")\n",
        "    # compute the std for the cross validation under the selected hyperparameter.\n",
        "    best_hp_vec = hpo_result.x\n",
        "    for i in range(len(order)): #key, value in hp_dict.items():\n",
        "        setattr(alg, order[i], best_hp_vec[i])\n",
        "    mse = cross_val_score(alg, X, y, cv = kfold, scoring = scoring)\n",
        "    cv_result = (hpo_result.fun, mse.std())\n",
        "    return hpo_result, cv_result   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIYOarN9B_wt",
        "colab_type": "text"
      },
      "source": [
        "Now, we evaluate the performance of the listed models. We consider doing a brief hyperparameter optimization for each model, so that a model is not unfairly judged because of one bad configuration. The good thing about using sklearn model is that sklearn always recommends a default set of hyperparameter configuration for a model. We use this default configuration as the starting point for our HPO. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tU_PGpduB_wu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "alg_names = {\"linr\" : \"Ridge Linear Regression\",\n",
        "             \"rf\" : \"Random Forest\",\n",
        "             \"xgb\" : \"Gradient Boosting Trees\",\n",
        "             \"svr_rbf\" : \"SVR with rbf kernel\",\n",
        "             \"svr_lin\" : \"SVR with linear kernel\"}\n",
        "def get_cv_results(n_calls = 15, \n",
        "                   n_random_starts = 5,\n",
        "                   hpo_optimizer = \"gp\", \n",
        "                   estimators = estimators,\n",
        "                   dict_spaces = dict_spaces, \n",
        "                   alg_names = alg_names,\n",
        "                   X = X_train, sd_X = sd_X_train, y = y_train):\n",
        "    hpo_results = {}\n",
        "    cv_results = {}\n",
        "    need_preproc = [\"linr\", \"svr_rbf\", \"svr_lin\"]\n",
        "    for key in estimators.keys():\n",
        "        if key in need_preproc:\n",
        "            hpo_results[key], cv_results[key] = auto_cv_score(estimators[key], \n",
        "                                                              dict_spaces[key], \n",
        "                                                              X = sd_X, y =y, \n",
        "                                                              n_calls = n_calls,\n",
        "                                                              n_random_starts = n_random_starts,\n",
        "                                                              hpo_optimizer =  hpo_optimizer)\n",
        "        else:\n",
        "            hpo_results[key], cv_results[key] = auto_cv_score(estimators[key], \n",
        "                                                              dict_spaces[key], \n",
        "                                                              X = X, y =y, \n",
        "                                                              n_calls = n_calls, \n",
        "                                                              n_random_starts = n_random_starts,\n",
        "                                                              hpo_optimizer =  hpo_optimizer)\n",
        "\n",
        "\n",
        "        \n",
        "    for key, value in cv_results.items():\n",
        "        print(\"{algo}'s cv MSE: mean = {mean:7.3f} (std = {std:7.3f})\" ).format(algo = alg_names[key],\n",
        "                                                                                mean = value[0],\n",
        "                                                                                std = value[1])\n",
        "      \n",
        "    return hpo_results, cv_results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIocwAUKB_wx",
        "colab_type": "text"
      },
      "source": [
        "The results for the cross validations are listed below. The mean and std arise from the randomness in cross validation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqevf7AgB_wx",
        "colab_type": "code",
        "outputId": "e018c377-0022-47a1-cb0e-8e61e466cb82",
        "colab": {}
      },
      "source": [
        "hpo_results, cv_results = get_cv_results()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/home/astar/py2/py2/local/lib/python2.7/site-packages/skopt/optimizer/optimizer.py:203: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  cat_eq = np.all(x_arr[cat_inds] == next_x_arr[cat_inds])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Gradient Boosting Trees's cv MSE: mean =  13.049 (std =   2.016)\n",
            "Random Forest's cv MSE: mean =  15.338 (std =   4.583)\n",
            "SVR with rbf kernel's cv MSE: mean =  44.311 (std =  20.466)\n",
            "SVR with linear kernel's cv MSE: mean =  80.038 (std =  99.001)\n",
            "Ridge Linear Regression's cv MSE: mean =  35.003 (std =   8.359)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcDf5dpQB_w0",
        "colab_type": "text"
      },
      "source": [
        "Looks like gradient boosting tree is the best so far, but random forest is not that bad also. Surprisingly, Ridge linear regression is better than SVR. Now let's take a closer look on the hyperparameter tunning processes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kr9_fUajB_w1",
        "colab_type": "text"
      },
      "source": [
        "We plot the convergence of the HPO in the following 3 plots. It could look like the improvement is small..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_PWOTZYB_w2",
        "colab_type": "code",
        "outputId": "5ac4353b-b579-482f-a81f-48dc354188a7",
        "colab": {}
      },
      "source": [
        "t = np.arange(1, 16, 1)\n",
        "def perf_min(lst):\n",
        "    lst_min = []\n",
        "    for i in range(len(lst)):\n",
        "        lst_min.append(np.min(lst[:(i+1)]))\n",
        "    lst_min = np.asarray(lst_min)\n",
        "    return lst_min\n",
        "\n",
        "perf={}\n",
        "for key, item in hpo_results.items():\n",
        "    perf[key] = perf_min(item.func_vals)\n",
        "# red dashes, blue squares and green triangles\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.plot(t, perf[\"xgb\"], 'r8-', label = \"GBT\")\n",
        "plt.plot(t, perf[\"rf\"], 'bs-', label = \"RF\")\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"CV MSE\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtoAAAF3CAYAAACbhOyeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n8KZidVja0xUAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f5e2a399410>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pVSs2bwB_w5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8f80zMWDB_w7",
        "colab_type": "code",
        "outputId": "6a4feaa4-c940-43cc-e374-60a4c5f75aa9",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(12,6))\n",
        "plt.plot(t, perf[\"linr\"], 'g^:', label = \"LR\")\n",
        "plt.plot(t, perf[\"svr_rbf\"], \"m2:\", label = \"SVR_RBF\")\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"CV MSE\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtEAAAF3CAYAAABjZBdpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f5e2a872110>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbUfd3APB_w_",
        "colab_type": "code",
        "outputId": "baba688f-c2a7-4872-dfdb-60b57bece75e",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(12,6))\n",
        "plt.plot(t, perf[\"svr_lin\"], \"k1-.\")\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"CV MSE\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtEAAAF3CAYAAABjZBdpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkBINAACDlGgAABikRAMAwKD/B/2YQqDOx7qSAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f5e2a984d50>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNHdYWEMB_xD",
        "colab_type": "text"
      },
      "source": [
        "If we look at the cv values during the HPO for each algorithms, we witness that there are quite a lot of fluctuation, and it is not straightforward to find hp config that beats the default hp config (the default's cv value is the first entry)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQuzw4DxB_xE",
        "colab_type": "code",
        "outputId": "70a650df-bef9-4fec-a973-2de5a2ea0764",
        "colab": {}
      },
      "source": [
        "print(\"Random Forest's cv values during HPO:\")\n",
        "print(hpo_results[\"rf\"].func_vals)\n",
        "print(\"-------------------------------\\n\")\n",
        "print(\"Gradient Boosting Trees's cv values during HPO:\")\n",
        "print(hpo_results[\"xgb\"].func_vals)\n",
        "print(\"-------------------------------\\n\")\n",
        "print(\"Ridged Linear Regression's cv values during HPO:\")\n",
        "print(hpo_results[\"linr\"].func_vals)\n",
        "print(\"-------------------------------\\n\")\n",
        "print(\"SVR (RBF)'s cv values during HPO:\")\n",
        "print(hpo_results[\"svr_rbf\"].func_vals)\n",
        "print(\"-------------------------------\\n\")\n",
        "print(\"SVR (linear)'s cv values during HPO:\")\n",
        "print(hpo_results[\"svr_lin\"].func_vals)\n",
        "print(\"-------------------------------\\n\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random Forest's cv values during HPO:\n",
            "[ 15.73616185  16.05607953  16.06705268  23.66464511  19.41560517\n",
            "  60.16747199  19.31377462  15.42660422  15.33801609  15.57829866\n",
            "  19.40809196  15.73757752  15.61385822  15.47838575  15.74637834]\n",
            "-------------------------------\n",
            "\n",
            "Gradient Boosting Trees's cv values during HPO:\n",
            "[   13.21070919  8313.75641223  4593.48883657  8777.66515427  5143.68985288\n",
            "  8781.94150632    13.21070919    13.42056323    13.40763469    13.34246484\n",
            "    13.40740184    13.51279207    13.49718591    13.3428335     13.04877033]\n",
            "-------------------------------\n",
            "\n",
            "Ridged Linear Regression's cv values during HPO:\n",
            "[ 35.03118184  49.13749051  36.06772057  38.11438471  35.33207604\n",
            "  35.3476264   35.00259731  35.01003685  35.09894886  35.00259777\n",
            "  35.00274645  35.00318818  35.00285404  35.00437912  35.0043692 ]\n",
            "-------------------------------\n",
            "\n",
            "SVR (RBF)'s cv values during HPO:\n",
            "[   48.56165431  1019.39160837   921.69505223  1018.83693862   147.13617409\n",
            "   924.90904458    52.05216795  1018.2767826     44.31143711    44.55153263\n",
            "    44.82189441    45.06521275    45.29168408    45.47270724    45.66043974]\n",
            "-------------------------------\n",
            "\n",
            "SVR (linear)'s cv values during HPO:\n",
            "[    80.03805807  13646.99761456     82.58841917    475.57563398\n",
            "    200.12702071    407.46893659     80.67625728     82.84922354\n",
            "     81.26184716     80.69094559     80.64381445     80.80780063\n",
            "     81.25468619     81.04905042     80.94835261]\n",
            "-------------------------------\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJH5pM6kB_xH",
        "colab_type": "text"
      },
      "source": [
        "Let's perform 50 additional iterations for our HPO. We use gaussian processes as our HP optimizers. It takes about 20 minutes for the computation below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRsyQe4bB_xH",
        "colab_type": "code",
        "outputId": "b930c5ba-ed6f-4319-afe5-a00f219a789d",
        "colab": {}
      },
      "source": [
        "# takes ~ 20 minutes...\n",
        "estimators[\"rf\"] = (RandomForestRegressor(), [\"n_estimators\", \n",
        "                                            \"max_features\",\n",
        "                                            \"max_depth\",\n",
        "                                            \"min_samples_split\"])\n",
        "\n",
        "further_rf_hpo = auto_cv_score(estimators[\"rf\"],\n",
        "                               dict_spaces[\"rf\"],\n",
        "                               X_train, y_train, \n",
        "                               kfold = kfold, \n",
        "                               scoring = scoring, \n",
        "                               default = False,\n",
        "                               hpo_optimizer = \"gp\", \n",
        "                               n_calls = 50, \n",
        "                               n_random_starts = 0,\n",
        "                               x0=hpo_results[\"rf\"].x_iters, \n",
        "                               y0=hpo_results[\"rf\"].func_vals, \n",
        "                               random_state=seed, \n",
        "                               verbose=False)\n",
        "\n",
        "estimators[\"xgb\"] = (XGBRegressor(), [\"learning_rate\",   \n",
        "                                      \"gamma\",\n",
        "                                      \"max_depth\",       \n",
        "                                      \"min_child_weight\",                          \n",
        "                                      \"max_delta_step\",\n",
        "                                      \"subsample\", \n",
        "                                      \"colsample_bytree\",\n",
        "                                      \"colsample_bylevel\",\n",
        "                                      \"reg_lambda\",\n",
        "                                      \"reg_alpha\", \"n_estimators\"])\n",
        "\n",
        "further_xgb_hpo = auto_cv_score(estimators[\"xgb\"],\n",
        "                               dict_spaces[\"xgb\"],\n",
        "                               X_train, y_train, \n",
        "                               kfold = kfold, \n",
        "                               scoring = scoring, \n",
        "                               default = False,\n",
        "                               hpo_optimizer = \"gp\", \n",
        "                               n_calls = 50, \n",
        "                               n_random_starts = 0,\n",
        "                               x0=hpo_results[\"xgb\"].x_iters, \n",
        "                               y0=hpo_results[\"xgb\"].func_vals, \n",
        "                               random_state=seed, \n",
        "                               verbose=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/home/astar/py2/py2/local/lib/python2.7/site-packages/skopt/optimizer/optimizer.py:208: UserWarning: The objective has been evaluated at this point before.\n",
            "  warnings.warn(\"The objective has been evaluated \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdxQSXs3B_xK",
        "colab_type": "text"
      },
      "source": [
        "Let's take a look at the best hyperparameter configurations we have obtained so far:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMRnFUyoB_xL",
        "colab_type": "code",
        "outputId": "611d442c-0f42-41f1-e436-b20268b59ded",
        "colab": {}
      },
      "source": [
        "best_rf = dict(zip(estimators[\"rf\"][1], further_rf_hpo[0].x))\n",
        "best_xgb = dict(zip(estimators[\"xgb\"][1], further_xgb_hpo[0].x))\n",
        "print(\"Our best hyperparameter configuration for random forest is\")\n",
        "for key, item in best_rf.items():\n",
        "    print(key + \": \" + str(item))\n",
        "print(\"-------------------------------------------------------------\")\n",
        "print(\"Our best hyperparameter configuration for gradient boosting is\")\n",
        "for key, item in best_xgb.items():\n",
        "    print(key + \": \" + str(item))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Our best hyperparameter configuration for random forest is\n",
            "n_estimators: 60\n",
            "max_features: 0.4\n",
            "min_samples_split: 2\n",
            "max_depth: None\n",
            "-------------------------------------------------------------\n",
            "Our best hyperparameter configuration for gradient boosting is\n",
            "reg_alpha: 0.948766094664\n",
            "colsample_bytree: 0.819748481326\n",
            "colsample_bylevel: 0.845517728008\n",
            "learning_rate: 0.0849090680093\n",
            "max_delta_step: 0.0\n",
            "min_child_weight: 2.37764760709\n",
            "n_estimators: 488\n",
            "subsample: 0.685707865\n",
            "reg_lambda: 1.9297111035\n",
            "max_depth: 4\n",
            "gamma: 0.218106329892\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ea6nuit7B_xO",
        "colab_type": "text"
      },
      "source": [
        "We witness the following convergence:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-LZg4FzB_xO",
        "colab_type": "code",
        "outputId": "009b00ff-2ed5-41bd-981f-a68a9b60d63f",
        "colab": {}
      },
      "source": [
        "final_xgb = perf_min(np.asarray(further_xgb_hpo[0].func_vals))\n",
        "final_rf = perf_min(np.asarray(further_rf_hpo[0].func_vals))\n",
        "tt = np.arange(1, 66, 1)\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.plot(tt, final_xgb, 'r8-', label = \"GBT\")\n",
        "plt.plot(tt, final_rf, 'bs-', label = \"RF\")\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"CV MSE\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtoAAAF3CAYAAACbhOyeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCNoAANDB/w8fS9sqbcHJiQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f5e1e10fa50>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyDZt3yzB_xR",
        "colab_type": "text"
      },
      "source": [
        "Finally, we evaluate the performance of our tuned GBT and RF on the test and validation datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SuT--7SZB_xS",
        "colab_type": "code",
        "outputId": "b1780e4a-ee73-4a02-b24f-1b8917a23574",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "# define the models with our best hyperparameters.\n",
        "best_rf_model = RandomForestRegressor(**best_rf)\n",
        "best_xgb_model = XGBRegressor(**best_xgb)\n",
        "# fit the xgb and rf with the whole of training dataset.\n",
        "best_rf_model.fit(X_train, y_train)\n",
        "best_xgb_model.fit(X_train, y_train)\n",
        "# predict\n",
        "y_rf_test = best_rf_model.predict(X_test)\n",
        "y_rf_valid = best_rf_model.predict(X_valid)\n",
        "y_xgb_test = best_xgb_model.predict(X_test)\n",
        "y_xgb_valid = best_xgb_model.predict(X_valid)\n",
        "# the MSEs\n",
        "xgb_test_mse = mean_squared_error(y_xgb_test, y_test)\n",
        "xgb_test_mae = mean_absolute_error(y_xgb_test, y_test)\n",
        "xgb_valid_mse = mean_squared_error(y_xgb_valid, y_valid)\n",
        "xgb_valid_mae = mean_absolute_error(y_xgb_valid, y_valid)\n",
        "rf_test_mse = mean_squared_error(y_rf_test, y_test)\n",
        "rf_test_mae = mean_absolute_error(y_rf_test, y_test)\n",
        "rf_valid_mse = mean_squared_error(y_rf_valid, y_valid)\n",
        "rf_valid_mae = mean_absolute_error(y_rf_valid, y_valid)\n",
        "# print out\n",
        "print(\"On the training set: \")\n",
        "print(\"GBT has cv MSE \" + str(13))\n",
        "print(\"Random Forest has cv MSE \" + str(15.25))\n",
        "print(\"-------------------------------------------\")\n",
        "print(\"On the test set: \")\n",
        "print(\"GBT has MSE \" + str(xgb_test_mse))\n",
        "print(\"GBT has MAE \" + str(xgb_test_mae))\n",
        "print(\"Random Forest has MSE \" + str(rf_test_mse))\n",
        "print(\"Random Forest has MAE \" + str(rf_test_mae))\n",
        "print(\"-------------------------------------------\")\n",
        "print(\"On the validation set: \")\n",
        "print(\"GBT has MSE \" + str(xgb_valid_mse))\n",
        "print(\"GBT has MAE \" + str(xgb_valid_mae))\n",
        "print(\"Random Forest has MSE \" + str(rf_valid_mse))\n",
        "print(\"Random Forest has MAE \" + str(rf_valid_mae))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "On the training set: \n",
            "GBT has cv MSE 13\n",
            "Random Forest has cv MSE 15.25\n",
            "-------------------------------------------\n",
            "On the test set: \n",
            "GBT has MSE 11.1693288366\n",
            "GBT has MAE 2.5768468448\n",
            "Random Forest has MSE 12.1476106446\n",
            "Random Forest has MAE 2.61338905759\n",
            "-------------------------------------------\n",
            "On the validation set: \n",
            "GBT has MSE 11.552476739\n",
            "GBT has MAE 2.54867379054\n",
            "Random Forest has MSE 11.0687909462\n",
            "Random Forest has MAE 2.5090241591\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FNnYgjaB_xU",
        "colab_type": "text"
      },
      "source": [
        "Here is the code for performing HPO, in the case when you are given a training and a test dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1R8SGDktB_xV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "from skopt import dummy_minimize, gp_minimize, gbrt_minimize, forest_minimize\n",
        "def auto_validate(estimator, \n",
        "                  dict_space, \n",
        "                  X_train, y_train, X_test, y_test,\n",
        "                  error_func = mean_squared_error, \n",
        "                  default = True,\n",
        "                  hpo_optimizer = \"dummy\", \n",
        "                  n_calls = 11, \n",
        "                  n_random_starts = 10,\n",
        "                  x0=None, y0=None, \n",
        "                  random_state=seed, \n",
        "                  verbose=False):\n",
        "    \"\"\"\n",
        "        auto_validate aims to compute the best hyperparameter configuration for an\n",
        "        sklearn classifer/regressor. It trains a model with a given hp config on the\n",
        "        training set (X_train, y_train), then tests the model on the test set \n",
        "        (X_test, y_test). It also works with xgboost.XGBClassifer or \n",
        "        xgboost.XGBRegressor, which are the sklearn wrappers for xgboost classifier\n",
        "        and regressor. \n",
        "        \n",
        "        TODO: sklearn wrapper for keras??        \n",
        "        \n",
        "        Input:\n",
        "        - estimator is an ordered pair.\n",
        "            -First entry is an sklearn estimator, (we assume that we can access each \n",
        "             hyperparameter through the estimator's attribute, namely \n",
        "             \"estimator.hyperparamter\")\n",
        "            -Second entry is the ordered array of the hyperparameters in the estimator.\n",
        "             It represents the desired order in the presentation of the hyperparameter \n",
        "             configuration.\n",
        "             \n",
        "        - dict_space is a dictionary.\n",
        "            -Keys = hyperparameters' names\n",
        "            -values = the respective ranges for the hyperparameters\n",
        "            \n",
        "        - (X_train, y_train) is the training set, and (X_test, y_test) is the testing set.\n",
        "        \n",
        "        - error_func is the sklearn metric used for measuring the error. By default, we use\n",
        "          scoring = \"mean_squared_error\". See sklearn.metric for more details.\n",
        "          \n",
        "        - default is True, if we compute the test error first with the default hyperparameter\n",
        "          config recommended by sklearn. default is False otherwise. It is recommended to \n",
        "          set it to be true, unless you are very sure on what you are doing.\n",
        "          \n",
        "        - hpo_optimizer is the hyperparameter optimizer we are using:\n",
        "            - hpo_optimizer == \"dummy\" for using random search.\n",
        "            - hpo_optimizer == \"gp\" for using gaussian process optimization.\n",
        "            - hpo_optimzier == \"gbrt\" for using the gradient boosting trees variant of SMAC.\n",
        "            - hpo_optimizer == \"rf\" for using SMAC.\n",
        "            - hpo_optimzier == \"et\" for using the extra trees variaint of SMAC.\n",
        "          In fact, the performance of the last four optimizers are kinda similar. \n",
        "          TODO: make hpo_optimizer == \"gs\" (grid search), and hpo_optimizer = \"tpe\", if\n",
        "          there is a point in making these.\n",
        "        \n",
        "        - n_calls = number of hyperparameter evaluations\n",
        "            - if default == True and hpo_optimzer != dummy, then n_calls must be \\geq 11.\n",
        "            - if default == False and hpo_optimzer != dummy, then n_calls must be \\geq 10.\n",
        "            - Otherwise, hpo_optimizer == dummy, and n_calls can be any positive integer.\n",
        "            \n",
        "        - x0 is the list of hyperparameter configs to test on before running our HPO, if \n",
        "          the corresponding y0 is empty. Otherwise, (x0, y0) serves as a warm start for \n",
        "          HPO.\n",
        "          \n",
        "        - random_state is the randome seed we are using.\n",
        "              \n",
        "            \n",
        "        Output:\n",
        "        - hpo_result: The result of the hyperparameter optimization.\n",
        "    \"\"\"\n",
        "    alg = estimator[0]\n",
        "    order = estimator[1]\n",
        "    name, space = dict_space.keys(), dict_space.values()\n",
        "    if set(order)!=set(name):\n",
        "        raise ValueError(\"The hyperparameters for the estimater and the hyperspace do not match.\")\n",
        "    idx = np.empty_like(order, dtype = \"int\")\n",
        "    order = np.asarray(order)\n",
        "    name = np.asarray(name)\n",
        "    #space = np.asarray(space)\n",
        "    for i in range(len(idx)):\n",
        "        #the index INDEX in name where name[INDEX] = order[i] = i th hyperp in order.\n",
        "        idx[i] = np.where(name==order[i])[0][0]\n",
        "    ordered_space = []\n",
        "    for i in range(len(idx)):\n",
        "        ordered_space.append(space[idx[i]])\n",
        "    if default:\n",
        "        x_default = []\n",
        "        for i in range(len(order)):\n",
        "            x_default.append(getattr(alg, order[i]))\n",
        "        if x0:\n",
        "            x0 = [x_default] + x0\n",
        "        else:\n",
        "            x0 = x_default\n",
        "    def validate_skopt(hp_vec):\n",
        "        for i in range(len(order)): #key, value in hp_dict.items():\n",
        "            setattr(alg, order[i], hp_vec[i])\n",
        "        alg.fit(X_train, y_train)\n",
        "        y_pred = alg.pred(X_test)\n",
        "        error = error_func(y_pred, y_test)\n",
        "        return error\n",
        "    if hpo_optimizer == \"dummy\":\n",
        "        hpo_result = dummy_minimize(validate_skopt, ordered_space, n_calls = n_calls, \n",
        "                                x0=x0, y0=y0, random_state=random_state, verbose=verbose)\n",
        "    elif hpo_optimizer == \"gp\":\n",
        "        hpo_result = gp_minimize(validate_skopt, ordered_space,\n",
        "                                 n_calls = n_calls, n_random_starts = n_random_starts,\n",
        "                             x0=x0, y0=y0, random_state=random_state, verbose=verbose)\n",
        "    elif hpo_optimizer == \"gbrt\":\n",
        "        hpo_result = gbrt_minimize(validate_skopt, ordered_space, \n",
        "                                   n_calls = n_calls, n_random_starts = n_random_starts,\n",
        "                             x0=x0, y0=y0, random_state=random_state, verbose=verbose)\n",
        "    elif hpo_optimizer == \"rf\":\n",
        "        hpo_result = forest_minimize(validate_skopt, ordered_space, base_estimator='RF', \n",
        "                                     n_calls = n_calls, n_random_starts = n_random_starts,\n",
        "                            x0=x0, y0=y0, random_state=random_state, verbose=verbose)\n",
        "    elif hpo_optimizer == \"et\":\n",
        "        hpo_result = forest_minimize(validate_skopt, ordered_space, base_estimator='ET', \n",
        "                                     n_calls = n_calls, n_random_starts = n_random_starts,\n",
        "                            x0=x0, y0=y0, random_state=random_state, verbose=verbose)\n",
        "    else:\n",
        "        raise ValueError(\"The hpo_optimizer must be one of the followings: dummy, gp, gbrt, rf, et.\")\n",
        "    return hpo_result"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}